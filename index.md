---
layout: default
title: Home
---

<style>
  a {
    color: #0066cc;
  }
  a:hover {
    color: #004499;
  }
</style>

<section class="intro">
  <div class="container">
    <h4 class="lead">Hello!</h4>
    <p class="lead">
      I'm Annie (she/her), a 3rd year PhD student at Northwestern University in the <a href="https://tsb.northwestern.edu/">Technology & Social Behavior</a> program, a dual PhD
      program in Computer Science and Communications.
      <br><br>
      I'm currently at the <a href="https://interactiveaudiolab.github.io/">Interactive Audio Lab</a>, advised by Dr. Bryan Pardo. My research interests lie at the intersection of audio, deep learning, human-computer interaction, and computational musicology.
      <br><br>
      Previously, I completed my B.S. in Electrical & Computer Engineering with a concentration in Media Arts at <a href="https://www.olin.edu/">Olin College of Engineering</a>. In my free time, you can find me walking around record shops, watching stand up, or playing bananagrams. As a native New Yorker (Queens!!!), I am also on the hunt for the best bagel in Chicago. Please email me if you find it.
      <br><br>
      You can reach me at anniechu [at] u.northwestern.edu
    </p>
  </div>
</section>

<section class="research-updates">
  <div class="container">
    <h4 class="lead">Updates</h4>
    <ul>
      <li>
        <span style="color: blue;"><strong>Sep 2025:</strong></span> 
        Our work <em>"Listening in the Age of the Algorithm: Bridging Musicology and HCI Methodologies"</em> was accepted at Clouds, Streams, and Ground (Truths) Conference, in March 2026.
      </li>
      <li>
        <span style="color: blue;"><strong>Summer 2025:</strong></span> 
        Interning at Adobe (Sound Design AI group) 
      </li>
      <li>
        <span style="color: blue;"><strong>July 2025:</strong></span> 
        "Sound Check: Auditing Recent Audio Dataset Practices," work led by <a href="https://sites.google.com/view/williamagnew?usp=sharing">William Agnew</a> was accepted into AIES 2025. Check out the <a href="https://arxiv.org/abs/2410.13114">paper.</a>
      </li>
      <li>
        <span style="color: blue;"><strong>June 2025:</strong></span> 
        "The Rhythm In Anything (TRIA): Audio-prompted Drums generation with masked language modeling," work led by <a href="https://oreillyp.github.io/">Patrick O'Reilly</a>, was accepted at ISMIR 2025. Check out the <a href="https://therhythminanything.github.io/">demo page.</a>
      </li>
      <li>
        <span style="color: blue;"><strong>April 2025:</strong></span> 
        Presented our work <em>Text2FX</em> at ICASSP 2025! 
        Check out the 
        <a href="https://arxiv.org/abs/2409.18847">paper (arXiv)</a> 
        and the 
        <a href="text2fx/">demo page</a>.
      </li>
    </ul>
  </div>
</section>